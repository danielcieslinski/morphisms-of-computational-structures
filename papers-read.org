* [[https://mdpi-res.com/d_attachment/philosophies/philosophies-03-00015/article_deploy/philosophies-03-00015.pdf][The Algebraic View of Computation: Implementation, Interpretation and Time]]
Attila Egri-Nagy
2018
14 Pages

Computational implementations are special relations between what is computed and what computes it.

A chain of emulation is ultimately grounded in an algebraic object, a full transformation semigroup.

Mathematically, emulation is defined by structure preserving maps (morphisms) between semigroups.

In contrast, interpretations are general functions with no morphic properties. They can be used to derive semantic content from computations.

Hierarchical structure imposed on a computational structure plays a similar semantic role.

Beyond bringing precision into the investigation, the algebraic approach also sheds light on the interplay between time and computation.

It is argued that for doing a philosophical investigation of computation, semigroup theory provides the right framework.

** 1. Semigroup — Composition Table of Computations

Roughly speaking, computation is a dynamical process governed by some rules.

*** 1.1 Generalizing Traditional Models of Computation

When we limit the tape or the number of calculation steps to a finite number, the halting problem ceases to be undecidable.

We restrict the Turing Machine to a finite capacity of memory and such a restricted Turing machine is said to be a finite state automaton.

Thought: Why isn’t it mapped to PDA or a 2-stack machine?

*** Definition of Semigroups

xy = z

We can say that x is combined with y results in z.

One interpretation is that two sequential events x and y happens resulting in the event z.

Alternatively, x can be an input, y a function and xy denotes a function application usually denoted as y(x)

Or, the same idea with different terminology, x is a state and y is a state-transition operator. This explains how to compute.

Semigroup: a set with an associative binary operation and the composition often called multiplication.

Principle 1: State event Abstraction

We can identify an event with its resulting state: state x is where we end up when event x happens, relative to a ground state. The ground state, in turn, corresponds to a neutral event that does not change any state.

#+BEGIN_QUOTE
Numbers measure size, groups measure symmetry.

M. Armstrong, Groups and Symmetry (1988)
#+END_QUOTE

The general idea of measuring is that we assign a mathematical object to the thing to be measured. In the everyday sense, the assigned object is a number, but for measuring more complicated properties, we need more structured objects. In this sense, we say that semigroups measure computation. Turing completeness is just a binary measure, hence the need for a more fine-grained scale. Thus, we can ask questions like “What is the smallest semigroup that can represent a particular computation?”

*** 1.3 Computation as Accretion of Structure

The idea of accretion has a clear algebraic description: the set of generators. These are elements of a group whose combinations can generate the whole table.

** 2. Computation and Time

*** 2.1 Different Times

An extreme difference between logical time and actual time is that computation can be suspended

*** 2.2 Not Enough Time

If there were infinite time available for computation, or equivalently, infinitely fast computers, we would not need to do any programming at all, brute-force would suffice (time travel even changes computability). A possible shortcut to the future would render the P vs. NP problem meaningless with enormous consequences.

*** 2.2 Timeless Computation?

Memory space can often be exchanged for time and the limit of this process is the lookup table, the precomputed result. Taking the abstraction process to its extreme, we can replace the 2D composition table with a 1D look up table with keys as pairs (x,y) and values xy.

Computation, completely stripped off any locality the process may have, is just an association of keys to values.

We can talk about static computational structures, composition tables, and we can also talk about computational processes, sequences of events tracing a path in the composition table.

If computation is information processing, then information is frozen computation.

For a Rubik’s Cube, one can specify a particular state either by describing the positions and orientations of the cubelets or by giving a sequence of moves from the initial ordered configuration. This is another example of the state-event abstraction (Principle 1).

** 3. Homomorphism — The Algebraic Notion of Implementation

“A physical system implements a given computation when the causal structure of the physical system mirrors the formal structure of the computation.”

*** 3.1 Homomorphisms
Homomorphism is a knowledge extension tool: we can apply knowledge about one system to another. It is a way to predict outcomes of events in one dynamical system based on what we known about what happens in another one, given that a homomorphic relationship has been established. This is also a general trick for problem solving, widely used in mathematics. When obtaining a solution is not feasible in one problem domain, we can use easier operations by transferring the problem to another domain — assuming that we can move between the domains with structure preserving maps.

What does it mean to be in a homomorphic relationship for computational structures? Using the composition table definition, we can now define their structure preserving maps. If in a systems S event x combined with event y yields the event z = xy, then by a homomorphism φ : S → T, then in another system T the outcome of φ(x) combined with φ(y) is bound to be φ(z) = φ(xy), so the following equation holds

φ(xy) = φ(x)φ(y)

On the left hand side, composition happens in S, while on the right hand side composition is done in T.

A distinguished class of homomorphisms is isomorphisms, where the correspondence is one-to-one. In other words, isomorphisms are strictly structure preserving, while homomorphisms can be structure forgetting down to the extreme of mapping everything to a single state and to the identity operation. The technical details can be complicated due to clustering states (surjective homomorphism) and by the need of turning around homomorphism we also consider homomorphic relations.

By turning around implementations we can define computational models. We can say that a physical system implements an abstract computer, or we can say that the abstract computer is a computational model of the physical system.

*** 3.2 Computers as Physical systems

Definition 1 (vague). Computers are physical systems that are homomorphic images of computational structures (semigroups).

The first definition begs the question, how can a physical system be an image of a homomorphism, i.e., a semigroup itself? How can we cross the boundary between the mathematical realm and the external reality? First, there is an easy but hypothetical answer. According to the Mathematical Universe Hypothesis, all physical systems are mathematical structures, so we never actually leave the mathematical realm.

Secondly, the implementation relation can be turned around. Implementation and modeling are the 2 directions of the same isomorphic relation. If T implements S, then S is a computational model of T. Again, we stay in the mathematical realm, we just need to study mappings between semigroups.

Definition 2. Computers are physical systems whose computational models are homomorphic images of semigroups.

This definition of computers is orthogonal to the problem of whether mathematics is an approximation or a perfect description of a physical reality, and the definition does not depend on how physical systems are characterized.

Biological systems are also good candidates for hosting computation, since they’re already doing some information processing. However, it is radically different from digital computation. The computation in digital computers is like toppling dominoes, a single sequence of chain reactions of bit-flips. Biological computation is done in a massively parallel way (e.g., all over in a cell), more in a statistical mode.

*** 3.3 Difficulty in Programming

*** 3.4 Interpretations

Computational implementation is a homomorphism, while an arbitrary function with no homomorphic properties is an interpretation. We can take a computational structure and assign freely some meaning to its elements, which we call the semantic content. Interpretations look more powerful since they can bypass limitations imposed by the morphic nature of implementations. However, since they are not necessarily structure preserving, the knowledge transfer is just one way. Changes in the underlying system may not be meaningful in the target system. If we ask a new question, then we have to devise a new encoding for the possible solutions.

For instance, reversible system can carry out irreversible computation by a carefully chose output encoding. A morphism can only produce irreversible systems out of irreversible(?) systems. This in turn demonstrates that today's computers are not based on the reversible laws of physics. From the logic gates up, we have proper morphic relations, but the gates themselves are not homomorphic images of the underlying physics. When destroying information, computers dissipate heat. Whether we can implement group computation with reversible transformations and hook on a non-homomorphic function to extract semantic content is an open engineering problem. In essence, the problem of reversible computation implementing programs with memory erasure is similar to trying to explain the arrow of time arising from the symmetrical laws of physics.

Throwing computing into reverse (2017) — M. P. Frank

** 4. High-Level Structure: Hierarchies

Composition and lookup tables are the “ultimate reality” of computation, but they are not adequate descriptions of practical computing. The low-level process in a digital computer, the systematic bit flips in a vast array of memory, is not very meaningful. The usefulness of a computation is expressed at several hierarchical layers above (e.g., computer architecture, operating system, and end user applications). 

A semigroup is seldom just a flat structure, its elements may have different roles. For example, if xy = z but yx = y (assuming x ≠ y ≠ z), then we say that x has no effect on y (leaves it fixed) while y turns x into z. There is an asymmetric relationship between x and y: y can influence x but not the other way around. This unidirectional influence gives rise to hierarchical structrues. It is actually better than that. According to the Krohn-Rhodes theory, every automaton can be emulated by a hierarchical combination of simpler automata. This is true even for inherently non-hierarchical automata built with feedback loops between its components. It is a surprising result of algebraic automata theory that recurrent networks can be rolled out to one-way hierarchies. These hierarchies can be thought as easy-to-use cognitive tools for understanding complex systems. They also give a framework for quantifying biological complexity.

The simpler components of a hierarchical decomposition are roughly of two kinds. Groups correspond to reversible combinatorial computation. Groups are also associated with isomorphisms (due to the existence of unique inverses), so their computation can also be viewed as pure data conversion. Semigroups, which fail to be groups, contain some irreversible computation, i.e., destructive memory storage.

** 5. Wild Considerations

The question of whether cognition is computational or not, might be the same as the question of whether mathematics is a perfect description of physical reality or is just an approximation of it. If it is just an approximation, then there is a possibility that cognition resides in physical properties that are left out.

A recurring question in philosophical conversations is the possibility of the same physical system realizing two different minds simultaneously. Let’s say n is the threshold for being a mind. You need at least n states for a computational structure to do so. Then suppose there is more than one way to produce a mind with n states, so the corresponding full transformation group T_n can have subsemigroups corresponding to several mind. We then need a physical system to implement T_n. Now, it is a possibility to have different embeddings into the same system, so the algebra might allow the possibility of two minds coexisting in the same physical system. However, it also implies that most subsystems will be shared or we need a bigger host with at least 2n states. If everything is shared, the embeddings can still be different, but then a symmetry operation could take one mind into the other. This is how far mathematics can go in answering the question. For scientific investigation, these questions are still out of scope. Simpler questions about the computation form a research program. For instance, What is the minimum number of states to implement a self-referential system? and, more generally, What are the minimal implementations of certain functionalities? and How many computational solutions are there for the same problem? These are engineering problems, but solutions for these may shed light on the more difficult questions about the possibilities and constraints of embedding cognition and consciousness into computers.

* 6. Summary

In the opinion of the author, philosophy should be ahead of mathematics, as it deals with more difficult questions, and it should not be bogged down by the shortcomings of terminology. In philosophy, we want to deal with the inherent complexity of the problem, not the incidental complexity caused by the chosen tool. The algebraic viewpoint provides a solid base for further philosophical investigations of the computational phenomena.

* Finite Computational Structures and Implementations
Attila Egri-Nagy
2016

12 Pages

** Abstract

What is computable with limited resources?
How can we verify the correctness of computations?
How to measure computational power with precision?

Despite the immense scientific and engineering progress in computing, we still have only partial answers to these questions. In order to make these problems more precise, we describe an abstract algebraic definition of classical computation, generalizing traditional models to semigroups.

** 1. Introduction

The exponential growth of the computing power of hardware (colloquially known as Moore’s Law) seems to be ended by reaching its physical and economical limits. In order to keep up technological development, producing more mathematical knowledge about digital computation is crucial for improving the efficiency of software.

** 2. Computational Structures

Our computers are physical devices ad the theory of computation is abstracted from physical processes. Mathematical models clearly define the notion of computation, but mapping the abstract computation back to teh physical realm is often considered problematic. It is argued that structure-preserving maps between computations work from one mathematical model to another just as well as from the abstract to the concrete physical implementation, easily crossing any ontological borderline one might assume between the two.

Since abstract algebra provides the required tools, suggestion is made for further abstractions to the models of computations to reach the algebraic level safe for discussing implementations. It is also suitable for capturing the hierarchical structure of computers. Finiteness and the abstract algebraic approach paint a picture where universal computation becomes relative and the ‘mathematical versus physical’ distinction less important.

First it is attempted to define computations and implementations purely as abstract functions, then the need for combining functions leads us to definition of computational structures.

*** 2.1 Computation as a function: input-output mappings

Starting from the human perspective, computation is a tool. We want a solution for some problem: the input is the question, the output is the answer. Formally, the input-output pairs are represented as a function f : X → Y, and computation is function evaluation f(x) = y, x ∈ X, y ∈ Y. As an implementation of f, we need a dynamical system whose behaviour can be modelled by another function g, which is essentially the same as f.

Computation can be described by a mapping
f : { 0, 1 }^ m → { 0, 1 }^n , m,n ∈ ℕ

According to the fundamental theorem of reversible computing, any finite function can be computed by an invertible function. This apparently contradicts the idea of implementation, that important properties of functions have to be preserved.

Embedding XOR:

[00] ↦ [0]0
[01] ↦ [1]1
[10] ↦ [1]0
[11] ↦ [0]1

Embedding FAN-OUT:

[0]0 ↦ [00]
0[1] ↦ [11]
10 ↦ 10
11 ↦ 01

into the same bijective function. By putting information into the abstract elements, any function can ‘piggyback’ even on the identity function.

These ‘tricks’ work by composing the actual computations with special input and output functions, that might have different properties. In reversible computing the readout operation may not be a reversible function.

*** 2.2 Computation as a process: state transitions

Focussing on the process view, what is the most basic unit of computation? A state transition: an event changes the current state of a system. A state is defined by a configuration of a system’s components, or some other distinctive properties the system may have.

Let’s say the current state is x, then event s happens changing the state to y. We might write y = s(x) emphasizing that s is a function, but it is better to write xs = y meaning that s happens in state x yielding state y. Why? Because combining events as they happen one after the other, e.g. xst = z, is easier to read following our left to right convention.

State-event abstraction: We can identify an event with its resulting state: state x is where we end up when event x happens.

According to the action interpretation, xs = y can be understood as event s changes the current state x to the next state y. But xs = y can also be read as event x combined with event s yields the composite event y, the event algebra interpretation. We can combine abstract events into longer sequences. These can also be considered as a sequence of instructions, i.e. an algorithm. These sequences of events should have the property of associativity

(xy)z = x(yz) for all abstract events x, y, z

since a given sequence xyz should be a well-defined algorithm.

We can put all event combinations into a table. These are the rules describing how to combine any two events.

Computational Structure: A finite set X and a rule for combining elements of X that assigns a value x' for each two-element sequence, written as xy = x', is a computational structure if (xy)z = x(yz) for all x, y, z ∈ X.

In mathematics, a set closed under an associative binary operation is an abstract algebraic structure called semigroup.

Computation is a process in time — an obvious assumption, since most of engineering and complexity studies are about doing computation in shorter time. Combining two events yield a third one (which can be the same), and we can continue with combining them to have an ordered sequence of events. This ordering may be referred as time. However, at the abstraction level of the state transition table, time is not essential. The table implicitly describes all possible sequences of events, it defines the rules how to combine any two events, but it is a timeless entity. This is similar to some controversial ideas in theoretical physics such as the idea of “The End of Time” by D. M. Barbour of Shape Dynamics fame.

*** 2.3 The computation spectrum

How are the function and the process view of computation related? They are actually the same. Given a computable function, we ca construct a computational structure capable of computing the function. An algorithm (a sequence of state transition events) takes an initial state (encoded input) into a final state (encoded output). The simplest way to achieve this is by a lookup table.

Lookup table semigroup: Let f : X → Y be a function, where X ∩ Y = ∅. Then the semigroup S = X ∩ Y ∩ {l} consists of resets X ∪ Y and the lookup operation l defined by xl = y if f(x) = y for all x ∈ X and ul = u for all u ∈ S \ X.

Is it associative? Let v ∈ X ∪ Y be an arbitrary reset element, and s, t ∈ S any element. Since the rightmost event is a reset, we have (st)v = v and s(tv) = sv = v. For (sv)l = vl = s(vl) since vl is also a reset. For (vl)l = vl, since l does not change anything in S \ X and v(ll) = vl since l is an idempotent (ll = l). Separating the domain and the codomain of f is crucial, for function X → X we can simply have a separate copy of elements of X. When trying to make it more economical associativity may not be easy to achieve.

Turning a computational structure into a function is also easy. Pick any algorithm (a composite event), and that is also a function from states to states.

Information storage and retrieval are forms of computation. By the same token computation can be considered as a general form of information storage and retrieval, where looking up the required piece of data may need many steps. We can say that if computation is information processing, then information is frozen computation.

*** 2.4 Traditional mathematical models of computation

From finite state automata, we abstract away the initial and accepting states. Those special states are needed only for the important application of recognizing languages. Input symbols of a finite state automaton are fully defined transformations (total functions) of its state set.

A transformation is a function f : X → X from a set to itself, and a transformation semigroup (X, S) of degree n is a collection S of transformations of an n-element set closed under function composition.

If we focus on the possible state transitions of a finite state automaton only, we get a transformation semigroup with a generator set corresponding to the input symbols. These semigroups are very special representations of abstract semigroups, where state transition is realized by composing functions.

If we focus on the possible state transitions of a finite state automaton only, we get a transformation semigroup with a generator set corresponding to the input symbols. These semigroups are very special representations of abstract semigroups, where state transition is realized by composing functions.

In general, if we take the models of computation that describe the detailed dynamics of computation, and remove all the model specific decorations, we get a semigroup.

*** 2.5 Computers: physical realizations of computation

Intuitively, a computer is a physical system whose dynamics at some level can be described as a computational structure. For any equation xy = z in the computational structure, we should be able to induce in the physical system an event corresponding to x and another one corresponding to y such that their overall effect corresponds to z. Algebraically, this special correspondence is a structure-preserving map, a homomorphism. If we want exact realizations, not just approximations, then we need stricter one-to-one mappings, isomorphisms. However, for computational structures we need to use relations instead of functions.

Isomorphic relations of computational structures: Let S and T be computational structures (semigroups). A relation ɸ : S → T is an isomorphic relation if it is:

1) homomorphic: ɸ(s)ɸ(t) ⊆ ɸ(st)
2) fully defined: ɸ(s) ≠ ∅ for all s ∈ Structures
3) lossless: ɸ(s) ∩ ɸ(t) ≠ ∅ ⇒ s = t
for all s, t ∈ S. We also say that T emulates, or implements S.

Homomorphic is the key property, it ensures that similar computation is done in T by matching individual state transitions. Here ɸ(s) and ɸ(t) are subsets of T (not just single elements), and ɸ(s)ɸ(t) denotes all possible state transitions induced by these subsets (element-wise product of two sets). Fully defined meas that we assign some state(s) of T for all elements of S, so we account for everything that can happen in S. In general, homomorphic maps are structure-forgetting, since we can map several states to a single one. Being lossless excludes loosing information about state transitions. In semigroup theory, isomorphic relations are called divisions, a special type of relational morphisms.

What happens if we turn an implementation around? It becomes a computational model.

Modelling of computational structures. Let S and T be computational structures (semigroups). A function µ : T → S is a modelling if it is:

1) homomorphism µ(u)µ(v) = µ(uv) for all u, v ∈ T,
2) onto: for all s ∈ S there exists a u ∈ T such that µ(u) = s.

We also say that S is a computational model of T. In algebra, functions of this kind are called surjective homomorphisms.

A modelling is a function, so it is fully defined. A modelling µ turned around µ^-1 is an implementation, and an implementation ɸ turned around is a modelling ɸ^-1. This is an asymmetric relation, naturally we assume that a model of any system is smaller in some sense than the system itself. Also, to implement a computational structure completely we need another structure at least as big.

According to the mathematical universal hypothesis, we have nothing more to do, since we covered mappings from one mathematical structure to another one. In practice, we do seem to have a distinction between mathematical models of computations and actual computers, since abstract models by definition are abstracted away from reality, they do not have any inherent dynamical force to carry out state transitions. Even pen and paper calculations require a driving force, the human hand and the pattern matching capabilities of the brain. But we can apply a simple strategy: we treat a physical system as a mathematical structure, regardless its ontological status. Building a computer then becomes the task of constructing an isomorphic relation.

Computer: An implementation of a computational structure by a physical system is a computer.

Anything that is capable of state transition can be used for some computation. The question is how useful that computation is? We can always map the target system’s mathematical model onto itself. In this sense the cosmos can be considered as a giant computer computing itself. However, this statement is a bit hollow since we don to have a complete mathematical model of the universe. Every physical system computes, at least its future states, but not everything does useful calculation. Much like entropy is heat not available for useful work. The same way as steam and combustion engines exploit physical processes to process materials and move things around, computers exploit physical process to transfer and transform information.

*** 2.6 Hierarchical structure

Huge state transition tables are not particularly useful to look at; they are like quark-level descriptions for trying to understand living organisms. Identifying substructures and their interactions is needed. Hierarchical levels of organizations provide an important way to understand computers. Information flow is limited to one-way only along a partial order, thus enabling functional abstraction. According to Krohn-Rhodes theory, any computational structure can be built by using destructive memory storage and the reversible computational structures in a hierarchical manner. The way the components are put together is the cascade product which is a substructure of the algebraic wreath product. The distinction between reversible and irreversible is sharp: there is no way to embed state collapsing into a permutation structure. Reversible computing seems to contradict this. The trick there is to put information into states and then selectively read off partial information from the resulting states. This selection of required information can be done by another computational structure. We can have a reversible computational structure on the top, and one at the bottom that implements the readout. We can have many transitions in the reversible part without a readout. Reversible implementations may prove to be decisive in terms of power efficiency of computers, but it does not erase the difference between reversible and irreversible computations.

Important to note that hierarchical decompositions are possible even when the computational structure is not hierarchical itself. on of the research directions is the study of how it is possible to understand loopback systems in a hierarchical manner.

*** 2.7 Universal computers

What is the difference between a piece of rock and a silicon chip? They are made of the same material, but they have different computational power. The rock only computes its next state (its temperature, all wiggling of its atoms), so the only computation we can map to it homomorphically is its own mathematical description. While the silicon chip admits other computational structures. General purpose processors are homomorphic images of universal computers.

** 3. Open problems

The main topics where further research needs to be done are:

1) exploring the space of possible computations
2) measuring finite computational power
3) computational correctness

*** 3.1 What are the possible computational structures and implementations?

Cataloguing stocktaking are basic human activities for answering the question What do we have exactly? For the classification of computational structures and implementations, we need to explore the space of all computational structures and their implementations, starting from the small examples. Looking at those is the same as asking the questions What can we compute with limited resources? What is computable with n states? This is a complementary approach to computational complexity, where the emphasis is on the growth rate of resource requirements.

*** 3.2 How to measure the amount of computational power?


Given an abstract or physical computer, what computations can it perform? The algebraic description gives a clear definition of emulation, when one computer can do the job of some other computer. This is a crude form of measuring computational power, in the sense of the ‘at least as much as’ relation. This way computational power can be measured on a partial order (the lattice of all finite semigroups).

The remaining problem is to bring some computation into the common denominator semigroup form. For example, if we have a finite piece of cellular automata grid, what can we calculate with it? If the cellular automata is universal and big enough we might be able to fit in a universal Turing machine that would do the required computation. However, we might be able to run a computation directly on the cellular automata instead of a bulky construct.

Extending the slogan, “Numbers measure size, groups measure symmetry.”, we can say that semigroups measure computation.

*** How can we trust computers?

Computations can differ by:

1) having different intermediate results
2) applying different operations
3) having different modular structure

*** 4. Conclusion

* Finite Computational Structures and Implementations: Semigroups and Morphic Relations

Attila Egri-Nagy
2017

18 pages

** 1 Introduction

Computational complexity studies the asymptotic behaviour of algorithms. Complementing that, it is here suggested to focus on small theoretical computing devices, and study of the possibilities of limited finite computations. Instead of asking what resources we need in order to solve bigger instances of a computational problem, we restrict the resources and ask what can we compute within those limitations. The practical benefit of such an approach is that we get to know the lowest number of states required to execute a given algorithm and having the minimal example are useful for low-level optimizations. Another example of such a reversed question is asking what is the total set of all possible solutions for a problem instead of looking for the single right solution. The mathematical formalism turns this into a well-defined combinatorial question, and the payoff could be that we will find solutions we had never thought of.

** 2 Computational Structures

Since abstract algebra provides the required tools, we suggest further abstractions to the models of computations to reach the algebraic level safe for discussing implementations. It is also suitable for capturing the hierarchical structure of computers. Finiteness and the abstract algebraic approach paint a picture where universal computation becomes relative and the ‘mathematical versus physical’ distinction less important.

*** 2.1 Computation as function: input-output mappings

*** 2.2 Computation as a process: state transitions

A composition table of groups that are semigroups with an identity and a unique inverse for each element is always a Latin square.

**** 2.2.1 The computation spectrum

*** 2.3 Traditional mathematical models of computation

Finite State Automata is considered to be a discrete dynamical system.

By a finite state automaton, we mean a triple (X, Σ, δ) where

1. X is the finite state set
2. Σ is the finite input alphabet
3. δ : X × Σ → X is the transition function
How is this a definition of a semigroup? For each state x ∈ X an input symbol σ ∈ Σ gives the resulting state δ(X, σ) = (δ(x_1, σ), δ(x_2, σ), …, δ(x_n, σ)). Therefore, input symbols of a finite state automaton are fully defined transfomaitons (total functions) of its state set.

A transformation is a function f : X → X from a set to itself, and a transformation semigroup (X, S) of degree n is a collection S of transformations of an n-element set closed under function composition.

If we focus on the possible state transitions of a finite state automaton only, we get a transformation semigroup with a generator set corresponding to the input symbol.s These semigroups are very special representations of abstract semigroups, where state transitions is realized by composing functions. It turns out that any semigroup can be represented as a transformation semigroup (Cayley’s Theorem for semigroups).

Transformation semigroup realization of the flip-flop semigroup. The set being transformed is simply  X = { 0, 1 } also called the set of states.

s0 = { 0 ↦ 0, 1 ↦ 0 }
s1 = { 0 ↦ 1, 1 ↦ 1 }
r = { 0 ↦ 0, 1 ↦ 1 }

The events can be denoted algebraically by listing the images s0 = [0, 0], s1 = [1, 1], r = [0, 1].

*** 2.4 Computers: physical realizations of computation


**** 2.4.1 Morphic relations of computational structures

First, we give an algebraic definition of computational implementations, then we justify the choices in the definitions by going through the alternatives.

Emulation, isomorphic relation of computational strucutres. Let S and T be computational structures (semigroups). A relation ɸ : S → T is an isomorphic relation if it is:

1. homomorphic: ɸ(s)ɸ(t) ⊆ ɸ(st)
2. fully defined: ɸ(s) ≠ ∅ for all s ∈ S
3. lossless: ɸ(s) ∩ ɸ(t) ≠ ∅ ⇒ s = t

Homomorphism is a fundamental algebraic concept often described by the equation:

ɸ(s)ɸ(t) = ɸ(st),

where the key idea is hidden in the algebraic generality. We have two semigroups S and T, in which the actions of computations are the composition of elements. In both semigroups these are denoted by juxtaposition of their elements. This obscures the fact that computations in S and in T are different. Writing ∘S for composition in S and ∘_T for composition in T we can make the homomorphism equation more transparent:

ɸ(s) ∘_T ɸ(t) = ɸ(s ∘_S t)

This shows the underlying idea clearly: it does not matter whether we convert the inputs in S to the corresponding inputs in T and do the computation, in T (left hand side), or do the computation in S then send the outputs in S to its counterpart in T (right hand side), we will get the same result. In the above definition, ɸ(s) and ɸ(t) are subsets of T (not just single elements), and ɸ(s)ɸ(t) denotes all possible state transitions induced by these subsets (element-wise product of two sets).

S : s ----------------------------> st
    |            t                  |
    |            |                  |
    | ɸ          | ɸ                | ɸ
    |            |                  |
    |            v                  |
    v           ɸ(t)                v
T : ɸ(s) ------------->ɸ(s)ɸ(t) = ɸ(st)

Fully defined means that we assign some state(s) of T for all elements of S, so we account for everything that can happen in S. This is just a technical, not a conceptual requirement, since we can always restricte a morphism to a substructure of S.

Being lossless excludes loosing information about state transitions. In general, homomorphic maps are structure-forgetting, since we can map several states to a single one. In case of s_1 ≠ s_2 and both s_1 and s_2 are sent to t in the target, the ability of distinguishing them is lost. For lossless correspondence we need one-to-one maps. For relations this requires the image sets to be disjoint. Varying these conditions we can have a classification of structure preserving correspondences between semigroups.

| | lossy (many-to-1) | lossless (1-to-1) |
|----+----------------+-------------------|
| relation (set-valued) | relational morphism | division |
| function (point-valued) | homomorphisms | isomorphism |

In semigroup theory, isomorphic relations are called divisions, a special type of relational morphism. This is a bit unfortunate terminology from computer science perspective, emulation instead of division, and morphic relation instead of relational morphism perhaps would be slightly better. In semigroup theory, relational morphisms are used for the purpose of simplifying proofs, not for any deep reasons. However, for describing computational implementations relations are necessary, since we need to be able to cluster states (e.g. micro versus macro states in a real physical settings).

*** 2.4.2 Computational models
Modelling of computational structures: Let S and T be computational structures (semigroups). A function µ : T → S is a modeling if it is:

1. homomorphism: µ(u)µ(v) = µ(uv) for all u, v ∈ T
2. onto: for all s ∈ S there exists a u ∈ T such that µ(u) = s

We also say that S is a computational model of T. In algebra, functions of this kind are called surjective homomorphisms.

In the case of ℤ_2 → ℤ_4, there are more divisions that isomorphisms. ℤ_2 is a quotient of ℤ_4, so ℤ_4 has a surjective homomorphism to ℤ_2. The division here is exactly that surmorphism turned around. Exactly this reversal of surjective homomorphisms was the original motivation for defining divisions.

Computer: An implementation of a computational structure by a physical system is a computer

*** 2.5 Hierarchical structure

Abstract state machines are generalizations of finite state machines. These models can be refined and coarsened forming a hierarchical succession, based on the same abstraction principles as in Krohn-Rhodes theory.

*** 2.6 Universal Computers

The full transformation semigroup of degree n (denoted by T_n) consists of all n^n transformations of an n-element set. These can be generated by compositions of three transformations: a cycle [1, 2, 3, …, n - 1, 0], a swap [1, 0, 2, 3, …, n - 1], and a collapsing [0, 0, 2, 3, …, n - 1]. Leaving out the state collapsing generator, we generate the symmetric group S_n, which is the universal structure of all permutations of n points.

** 3 Finite computation — research questions and results

Some practical problems of finite computation which are the difficult ones, that were not really in the focus of mathematical research:

1. What are the possible computational structures and implementations?
2. How to measure the amount of computational power?
3. How can we trust computers?

*** 3.1 Enumeration and classification of computational structures

The method of enumerating certain types of semigroups by enumerating all subsemigroups of relatively universal semigroup of that type has been applied to a wider class of semigroups, called diagram semigroups. These generalize function composition to other combinatorial structures (partial functions, binary relations, partitions, etc.), while keeping the possibility of representing the semigroup operation as stacking diagrams. These can be considered as ‘unconventional’ mathematical models of computations (e.g. computing with binary relations or partitions instead of functions). The existence of different types of computers leads to the problem of comparing their power.

*** 3.2 Measuring finite computational power

For an abstract semigroup, finding the minimal number of states n such that it embeds into the full transformation semigroup T_n is the same of finding the minimal number of states such that the given computation can be realized. This state minimization is an important engineering problem. It is not to be mistaken with the finite state automata minimization problem, where the equivalence is defined by recognizing the semi regular language, not by isomorphic relation.

*** 3.3 Algorithmic solution spaces and computational correctness

The simplest definition of a computational task is that we want to produce output from some input data. How many different ways are there for completing a particular task? The answer is infinity, unless we prohibit wasteful computations and give a clear definition of being different. Computational complexity distinguishes between classes of algorithms based on their space and time requirements. This is only one aspect of comparing solutions, since there might be different algorithms with very similar performance characteristics (e.g. bubble sort and insertion sort). Therefore we propose to study the set of all solutions more generally.

When are two solutions really different? The differences can be on the level of implementation or of the algorithm specification. Informally we can say that computations can differ by their:

1. intermediate results
2. applied operations
3. modular structure
4. or by any combination of these

An algorithmic solution space is a set of computer programs solving a computational problem, i.e. realizing a function described by input-output mappings.

* Algebraic Models for Understanding: Coordinate Systems and Cognitive Empowerment
C. Lev Nehaniv
16 pages
1997

** Abstract

We identify certain formal algebraic models affording understanding (including positional number systems, conservation laws in physics, and spatial coordinate systems) that have empowered humans when we have augmented ourselves using them. We survey how, by explicit mathematical constructions, that such algebraic models can be algorithmically derived for all finite-state systems and give examples illustrating this including coordinates for the rigid symmetries of a regular polygon, and recovery of the decimal expansion and coordinates arising from conserved quantities in physics.

Coordinate systems derived by algebra or computation for affordances of the understanding and manipulation of physical and conceptual worlds are thus a ‘natural’ step in the use of ‘tools’ by biological systems as they/we learn to modify selves and identities appropriately and dynamically.

* Introduction

Throughout the history of science, the greatest advances have come about when human beings came to realizations about how to think about their subject “correctly”. The deepest such advances have resulted from the advent of appropriate coordinate systems for a phenomenon to be understood. Some obvious and familiar examples include the decimal or binary expansions of the real numbers, Cartesian coordinate in analytic geometry, Galilean or Newtonian spatial coordinates, the periodic table in chemistry, conservation laws in physics, locally Euclidean coordinates in Riemannian geometry, and more recently, the idea of object-orientation.

Our chief claims in this paper are that:

1) such appropriate coordinate systems generally share certain crucial properties that explain their usefulness
2) such coordinate systems for understanding and manipulating a given system at least in the case of finite-state systems (such as our present day digital computers) can be generated algorithmically for use by humans or machines.

Such models for understanding may empower a human being to manipulate real, abstract, or synthetic worlds. It is important to note that to exploit or access formal models constructed with the aid of computers, a human being need not understand the algebraic theory behind their construction and should be free from such intellectual and computational encumbrances; however, mastering manipulation and application of a coordinate system e.g. the decimal number system may require some effort.

** 2 Properties of Understanding in Coordinates

When we reflect on how we understand a system using any of the historically useful coordinate systems mentioned above, certain properties of the coordinate systems are evident:

- *Globality*: we have some sort of description of essential characteristics of the system and can describe or predict how it will change with occurrences of events important for that system.

- *Hierarchy*: Except in the simplest cases, the whole is broken down into component parts, which themselves may consist of other parts, and so on, resulting in an ordering that encodes the dependencies among parts. Information from higher levels of the hierarchy gives good approximate knowledge, while “lower order” details are fleshed at subordinate levels

- *Simplicity of Components*: The smallest component parts are by themselves easy to understand

- *Covering*: We have implicitly or explicitly a knowledge of how to map the coordinate representation of the system to the system we are trying to understand.

A non-required property is that of a one-to-one correspondence between possible coordinate states and states of the system. Indeed, there is usually a many-to-one relationship between representations in coordinates and states of the system (as for example, with real numbers and their decimal representations).

Taking these properties as axiomatic for appropriate coordinate systems on understanding, we note that such understanding of a system is something quite different from knowledge (but may be related to such knowledge) of how the original system can be built or efficiently emulated, nor is it the same as knowledge of how the system is really structured. It is the utility afforded by such coordinate systems as tools for understanding which interests humans, but we shall also be concerned with the nature of affordance in cognitive (and physical) environments that are created, affected, or manipulated by these systems, as well as with explicit methods to construct these systems.