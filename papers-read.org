* [[https://mdpi-res.com/d_attachment/philosophies/philosophies-03-00015/article_deploy/philosophies-03-00015.pdf][The Algebraic View of Computation: Implementation, Interpretation and Time]]
Attila Egri-Nagy
2018
14 Pages

Computational implementations are special relations between what is computed and what computes it.

A chain of emulation is ultimately grounded in an algebraic object, a full transformation semigroup.

Mathematically, emulation is defined by structure preserving maps (morphisms) between semigroups.

In contrast, interpretations are general functions with no morphic properties. They can be used to derive semantic content from computations.

Hierarchical structure imposed on a computational structure plays a similar semantic role.

Beyond bringing precision into the investigation, the algebraic approach also sheds light on the interplay between time and computation.

It is argued that for doing a philosophical investigation of computation, semigroup theory provides the right framework.

** 1. Semigroup — Composition Table of Computations

Roughly speaking, computation is a dynamical process governed by some rules.

*** 1.1 Generalizing Traditional Models of Computation

When we limit the tape or the number of calculation steps to a finite number, the halting problem ceases to be undecidable.

We restrict the Turing Machine to a finite capacity of memory and such a restricted Turing machine is said to be a finite state automaton.

Thought: Why isn’t it mapped to PDA or a 2-stack machine?

*** Definition of Semigroups

xy = z

We can say that x is combined with y results in z.

One interpretation is that two sequential events x and y happens resulting in the event z.

Alternatively, x can be an input, y a function and xy denotes a function application usually denoted as y(x)

Or, the same idea with different terminology, x is a state and y is a state-transition operator. This explains how to compute.

Semigroup: a set with an associative binary operation and the composition often called multiplication.

Principle 1: State event Abstraction

We can identify an event with its resulting state: state x is where we end up when event x happens, relative to a ground state. The ground state, in turn, corresponds to a neutral event that does not change any state.

#+BEGIN_QUOTE
Numbers measure size, groups measure symmetry.

M. Armstrong, Groups and Symmetry (1988)
#+END_QUOTE

The general idea of measuring is that we assign a mathematical object to the thing to be measured. In the everyday sense, the assigned object is a number, but for measuring more complicated properties, we need more structured objects. In this sense, we say that semigroups measure computation. Turing completeness is just a binary measure, hence the need for a more fine-grained scale. Thus, we can ask questions like “What is the smallest semigroup that can represent a particular computation?”

*** 1.3 Computation as Accretion of Structure

The idea of accretion has a clear algebraic description: the set of generators. These are elements of a group whose combinations can generate the whole table.

** 2. Computation and Time

*** 2.1 Different Times

An extreme difference between logical time and actual time is that computation can be suspended

*** 2.2 Not Enough Time

If there were infinite time available for computation, or equivalently, infinitely fast computers, we would not need to do any programming at all, brute-force would suffice (time travel even changes computability). A possible shortcute to the future would render the P vs. NP problem meaningless with enormous consequences.

*** 2.2 Timeless Computation?

Memory space can often be exchanged for time and the limit of this process is the lookup table, the precomputed result. Taking the abstraction process to its extreme, we can replace the 2D composition table with a 1D look up table with keys as pairs (x,y) and values xy.

Computation, completely stripped off any locality the process may have, is just an association of keys to values.

We can talk about static computational structures, composition tables, and we can also talk about computational processes, sequences of events tracing a path in the composition table.

If computation is information processing, then information is frozen computation.

For a Rubik’s Cube, one can specify a particular state either by describing the positions and orientations of the cubelets or by giving a sequence of moves from the initial ordered configuration. This is another example of the state-event abstraction (Principle 1).

** 3. Homomorphism — The Algebraic Notion of Implementation

“A physical system implements a given computation when the causal structure of the physical system mirrors the formal structure of the computation.”

*** 3.1 Homomorphisms
Homomorphism is a knowledge extension tool: we can apply knowledge about one system to another. It is a way to predict outcomes of events in one dynamical system based on what we known about what happens in another one, given that a homomorphic relationship has been established. This is also a general trick for problem solving, widely used in mathematics. When obtaining a solution is not feasible in one problem domain, we can use easier operations by transferring the problem to another domain — assuming that we can move between the domains with structure preserving maps.

What does it mean to be in a homomorphic relationship for computational structures? Using the composition table definition, we can now define their structure preserving maps. If in a systems S event x combined with event y yields the event z = xy, then by a homomorphism φ : S → T, then in another system T the outcome of φ(x) combined with φ(y) is bound to be φ(z) = φ(xy), so the following equation holds

φ(xy) = φ(x)φ(y)

On the left hand side, composition happens in S, while on the right hand side composition is done in T.

A distinguished class of homomorphisms is isomorphisms, where the correspondence is one-to-one. In other words, isomorphisms are strictly structure preserving, while homomorphisms can be structure forgetting down to the extreme of mapping everything to a single state and to the identity operation. The technical details can be complicated due to clustering states (surjective homomorphism) and by the need of turning around homomorphism we also consider homomorphic relations.

By turning around implementations we can define computational models. We can say that a physical system implements an abstract computer, or we can say that the abstract computer is a computational model of the physical system.

*** 3.2 Computers as Physical systems

Definition 1 (vague). Computers are physical systems that are homomorphic images of computational structures (semigroups).

The first definition begs the questoin, how can a physical system be an image of a homomorphism, i.e., a semigroup itself? How can we cross the boundary between the mathematical realm and the external reality? First, there is an easy but hypothetical answer. According to the Mathematical Universe Hypothesis, all physical systems are mathematical structures, so we never actually leave the mathematical realm.

Secondly, the implementation relation can be turned around. Implementation and modeling are the 2 directions of the same isomorphic relation. If T implements S, then S is a computational model of T. Again, we stay in the mathematical realm, we just need to study mappings between semigroups.

Definition 2. Computers are physical systems whose computational models are homomorphic images of semigroups.

This definition of computers is orthogonal to the problem of whether mathematics is an approximation or a perfect description of a physical reality, and the definition does not depend on how physical systems are characterized.

Biological systems are also good candidates for hosting computation, since they’re already doing some information processing. However, it is radically different from digital computation. The computation in digital computers is like toppling dominoes, a single sequence of chain reactions of bit-flips. Biological computation is done in a massively parallel way (e.g., all over in a cell), more in a statistical mode.

*** 3.3 Difficulty in Programming

*** 3.4 Interpretations

Computational implementation is a homomorphism, while an arbitrary function with no homomorphic properties is an interpretation. We can take a computational structure and assign freely some meaning to its elements, which we call the semantic content. Interpretations look more powerful since they can bypass limitations imposed by the morphic nature of implementations. However, since they are not necessarily structure preserving, the knowledge transfer is just one way. Changes in the underlying system may not be meaningful in the target system. If we ask a new question, then we have to devise a new encoding for the possible solutions.

For instance, reversible system can carry out irreversible computation by a carefully chose output encoding. A morphism can only produce irreversible systems out of irreversible(?) systems. This in turn demonstrates that today's computers are not based on the reversible laws of physics. From the logic gates up, we have proper morphic relations, but the gates themselves are not homomorphic images of the underlying physics. When destroying information, computers dissipate heat. Whether we can implement group computation with reversible transformations and hook on a non-homomorphic function to extract semantic content is an open engineering problem. In essence, the problem of reversible computation implementing programs with memory erasure is similar to trying to explain the arrow of time arising from the symmetrical laws of physics.

Throwing computing into reverse (2017) — M. P. Frank

** 4. High-Level Structure: Hierarchies

Composition and lookup tables are the “ultimate reality” of computation, but they are not adequate descriptions of practical computing. The low-level process in a digital computer, the systematic bit flips in a vast array of memory, is not very meaningful. The usefulness of a computation is expressed at several hierarchical layers above (e.g., computer architecture, operating system, and end user applications). 

A semigroup is seldom just a flat structure, its elements may have different roles. For example, if xy = z but yx = y (assuming x ≠ y ≠ z), then we say that x has no effect on y (leaves it fixed) while y turns x into z. There is an asymmetric relationship between x and y: y can influence x but not the other way around. This unidirectional influence gives rise to hierarchical structrues. It is actually better than that. According to the Krohn-Rhodes theory, every automaton can be emulated by a hierarchical combination of simpler automata. This is true even for inherently non-hierarchical automata built with feedback loops between its components. It is a surprising result of algebraic automata theory that recurrent networks can be rolled out to one-way hierarchies. These hierarchies can be thought as easy-to-use cognitive tools for understanding complex systems. They also give a framework for quantifying biological complexity.

The simpler components of a hierarchical decomposition are roughly of two kinds. Groups correspond to reversible combinatorial computation. Groups are also associated with isomorphisms (due to the existence of unique inverses), so their computation can also be viewed as pure data conversion. Semigroups, which fail to be groups, contain some irreversible computation, i.e., destructive memory storage.

** 5. Wild Considerations

The question of whether cognition is computational or not, might be the same as the question of whether mathematics is a perfect description of physical reality or is just an approximation of it. If it is just an approximation, then there is a possibility that cognition resides in physical properties that are left out.

A recurring question in philosophical conversations is the possibility of the same physical system realizing two different minds simultaneously. Let’s say n is the threshold for being a mind. You need at least n states for a computational structure to do so. Then suppose there is more than one way to produce a mind with n states, so the corresponding full transformation group T_n can have subsemigroups corresponding to several mind. We then need a physical system to implement T_n. Now, it is a possibility to have different embeddings into the same system, so the algebra might allow the possibility of two minds coexisting in the same physical system. However, it also implies that most subsystems will be shared or we need a bigger host with at least 2n states. If everything is shared, the embeddings can still be different, but then a symmetry operation could take one mind into the other. This is how far mathematics can go in answering the question. For scientific investigation, these questions are still out of scope. Simpler questions about the computation form a research program. For instance, What is the minimum number of states to implement a self-referential system? and, more generally, What are the minimal implementations of certain functionalities? and How many computational solutions are there for the same problem? These are engineering problems, but solutions for these may shed light on the more difficult questions about the possibilities and constraints of embedding cognition and consciousness into computers.

* 6. Summary

In the opinion of the author, philosophy should be ahead of mathematics, as it deals with more difficult questions, and it should not be bogged down by the shortcomings of terminology. In philosophy, we want to deal with the inherent complexity of the problem, not the incidental complexity caused by the chosen tool. The algebraic viewpoint provides a solid base for further philosophical investigations of the computational phenomena.
